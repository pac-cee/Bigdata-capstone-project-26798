{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßπ Data Cleaning and Preprocessing\n",
    "\n",
    "**Project**: Cryptocurrency Market Intelligence System  \n",
    "**Author**: [Your Name]  \n",
    "**Course**: INSY 8413 | Introduction to Big Data Analytics  \n",
    "**Date**: July 26, 2025\n",
    "\n",
    "## üéØ Objectives\n",
    "1. Clean and validate collected cryptocurrency data\n",
    "2. Handle missing values and outliers\n",
    "3. Create basic derived features\n",
    "4. Prepare data for advanced analysis\n",
    "\n",
    "## üìã Data Quality Checks\n",
    "- Remove duplicates and invalid records\n",
    "- Validate price data consistency (high ‚â• low, close within range)\n",
    "- Handle missing values appropriately\n",
    "- Detect and treat outliers\n",
    "- Add basic technical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom modules\n",
    "from data_processor import CryptoDataProcessor\n",
    "from utils import CRYPTO_SYMBOLS, print_data_summary\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")\n",
    "print(f\"üïê Data cleaning started at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Initialize Data Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data processor\n",
    "processor = CryptoDataProcessor()\n",
    "\n",
    "# Define our analysis parameters\n",
    "SYMBOLS = ['BTC', 'ETH', 'BNB', 'ADA', 'SOL']\n",
    "INTERVALS = ['5m', '1h']\n",
    "\n",
    "print(\"üîß Data processor initialized!\")\n",
    "print(f\"üìä Symbols to process: {', '.join(SYMBOLS)}\")\n",
    "print(f\"‚è∞ Intervals to process: {', '.join(INTERVALS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Data Quality Assessment\n",
    "\n",
    "Let's first examine the quality of our raw data before cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess data quality for each cryptocurrency\n",
    "quality_summary = []\n",
    "\n",
    "print(\"üîç ASSESSING RAW DATA QUALITY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for symbol in SYMBOLS:\n",
    "    print(f\"\\nüìà {CRYPTO_SYMBOLS[symbol]} ({symbol})\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for interval in INTERVALS:\n",
    "        # Load raw data\n",
    "        raw_df = processor.load_raw_data(symbol, interval)\n",
    "        \n",
    "        if raw_df is not None and not raw_df.empty:\n",
    "            # Calculate quality metrics\n",
    "            total_records = len(raw_df)\n",
    "            missing_values = raw_df.isnull().sum().sum()\n",
    "            duplicates = raw_df.duplicated().sum()\n",
    "            \n",
    "            # Check for invalid price data\n",
    "            invalid_high_low = (raw_df['high'] < raw_df['low']).sum()\n",
    "            invalid_close = ((raw_df['close'] > raw_df['high']) | (raw_df['close'] < raw_df['low'])).sum()\n",
    "            negative_prices = (raw_df[['open', 'high', 'low', 'close']] <= 0).any(axis=1).sum()\n",
    "            \n",
    "            quality_metrics = {\n",
    "                'symbol': symbol,\n",
    "                'interval': interval,\n",
    "                'total_records': total_records,\n",
    "                'missing_values': missing_values,\n",
    "                'duplicates': duplicates,\n",
    "                'invalid_high_low': invalid_high_low,\n",
    "                'invalid_close': invalid_close,\n",
    "                'negative_prices': negative_prices,\n",
    "                'data_quality_score': 100 - ((missing_values + duplicates + invalid_high_low + invalid_close + negative_prices) / total_records * 100)\n",
    "            }\n",
    "            \n",
    "            quality_summary.append(quality_metrics)\n",
    "            \n",
    "            print(f\"  {interval:3} | Records: {total_records:>6,} | Missing: {missing_values:>4} | Duplicates: {duplicates:>4} | Quality: {quality_metrics['data_quality_score']:.1f}%\")\n",
    "        else:\n",
    "            print(f\"  {interval:3} | ‚ùå No data found\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "quality_df = pd.DataFrame(quality_summary)\n",
    "print(f\"\\nüìã Overall data quality assessment completed for {len(quality_df)} datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Data Cleaning Process\n",
    "\n",
    "Now let's clean all the data using our comprehensive cleaning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all cryptocurrency data\n",
    "print(\"üßπ STARTING DATA CLEANING PROCESS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "processed_data = processor.process_all_data(SYMBOLS, INTERVALS)\n",
    "\n",
    "print(\"\\n‚úÖ Data cleaning completed for all cryptocurrencies!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Cleaned Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create processing summary\n",
    "summary_df = processor.create_analysis_summary(processed_data)\n",
    "\n",
    "print(\"üìä CLEANED DATA SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Calculate overall statistics\n",
    "total_records = summary_df['records'].sum()\n",
    "avg_quality = quality_df['data_quality_score'].mean()\n",
    "total_missing = summary_df['missing_values'].sum()\n",
    "\n",
    "print(f\"\\nüìà OVERALL STATISTICS\")\n",
    "print(f\"Total Records Processed: {total_records:,}\")\n",
    "print(f\"Average Data Quality: {avg_quality:.1f}%\")\n",
    "print(f\"Remaining Missing Values: {total_missing}\")\n",
    "print(f\"Date Range: {summary_df['date_start'].min()} to {summary_df['date_end'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Data Visualization\n",
    "\n",
    "Let's visualize the cleaned data to ensure quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization of cleaned data\n",
    "fig, axes = plt.subplots(3, 2, figsize=(20, 15))\n",
    "fig.suptitle('Cleaned Cryptocurrency Data Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Price trends for hourly data\n",
    "ax1 = axes[0, 0]\n",
    "for i, symbol in enumerate(SYMBOLS):\n",
    "    if symbol in processed_data and '1h' in processed_data[symbol]:\n",
    "        df = processed_data[symbol]['1h']\n",
    "        ax1.plot(df.index, df['close'], label=f'{symbol}', alpha=0.8)\n",
    "\n",
    "ax1.set_title('Price Trends (Hourly Data)', fontweight='bold')\n",
    "ax1.set_ylabel('Price (USD)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Volume analysis\n",
    "ax2 = axes[0, 1]\n",
    "volume_data = []\n",
    "for symbol in SYMBOLS:\n",
    "    if symbol in processed_data and '1h' in processed_data[symbol]:\n",
    "        df = processed_data[symbol]['1h']\n",
    "        volume_data.append(df['volume'].mean())\n",
    "    else:\n",
    "        volume_data.append(0)\n",
    "\n",
    "bars = ax2.bar(SYMBOLS, volume_data, color=sns.color_palette(\"husl\", len(SYMBOLS)))\n",
    "ax2.set_title('Average Trading Volume', fontweight='bold')\n",
    "ax2.set_ylabel('Volume')\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, volume_data):\n",
    "    if value > 0:\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "                f'{value:.0e}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 3: Volatility comparison\n",
    "ax3 = axes[1, 0]\n",
    "volatility_data = []\n",
    "for symbol in SYMBOLS:\n",
    "    if symbol in processed_data and '1h' in processed_data[symbol]:\n",
    "        df = processed_data[symbol]['1h']\n",
    "        volatility = df['volatility_7d'].mean() * 100  # Convert to percentage\n",
    "        volatility_data.append(volatility)\n",
    "    else:\n",
    "        volatility_data.append(0)\n",
    "\n",
    "bars = ax3.bar(SYMBOLS, volatility_data, color=sns.color_palette(\"husl\", len(SYMBOLS)))\n",
    "ax3.set_title('Average 7-Day Volatility', fontweight='bold')\n",
    "ax3.set_ylabel('Volatility (%)')\n",
    "\n",
    "# Plot 4: Returns distribution for BTC\n",
    "ax4 = axes[1, 1]\n",
    "if 'BTC' in processed_data and '1h' in processed_data['BTC']:\n",
    "    btc_returns = processed_data['BTC']['1h']['returns_1d'].dropna() * 100\n",
    "    ax4.hist(btc_returns, bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "    ax4.axvline(btc_returns.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {btc_returns.mean():.2f}%')\n",
    "    ax4.set_title('BTC Hourly Returns Distribution', fontweight='bold')\n",
    "    ax4.set_xlabel('Returns (%)')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Data quality scores\n",
    "ax5 = axes[2, 0]\n",
    "quality_by_symbol = quality_df.groupby('symbol')['data_quality_score'].mean()\n",
    "bars = ax5.bar(quality_by_symbol.index, quality_by_symbol.values, \n",
    "               color=sns.color_palette(\"husl\", len(quality_by_symbol)))\n",
    "ax5.set_title('Data Quality Scores by Cryptocurrency', fontweight='bold')\n",
    "ax5.set_ylabel('Quality Score (%)')\n",
    "ax5.set_ylim(95, 100)\n",
    "\n",
    "# Add value labels\n",
    "for bar, value in zip(bars, quality_by_symbol.values):\n",
    "    ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "             f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 6: Record counts by interval\n",
    "ax6 = axes[2, 1]\n",
    "interval_counts = summary_df.groupby('interval')['records'].sum()\n",
    "colors = ['skyblue', 'lightcoral']\n",
    "wedges, texts, autotexts = ax6.pie(interval_counts.values, labels=interval_counts.index, \n",
    "                                   autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "ax6.set_title('Data Distribution by Interval', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate cleaned data quality\n",
    "print(\"üîç FINAL DATA QUALITY VALIDATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "validation_results = []\n",
    "\n",
    "for symbol in SYMBOLS:\n",
    "    for interval in INTERVALS:\n",
    "        if symbol in processed_data and interval in processed_data[symbol]:\n",
    "            df = processed_data[symbol][interval]\n",
    "            \n",
    "            # Validation checks\n",
    "            checks = {\n",
    "                'symbol': symbol,\n",
    "                'interval': interval,\n",
    "                'total_records': len(df),\n",
    "                'no_missing_critical': df[['open', 'high', 'low', 'close']].isnull().sum().sum() == 0,\n",
    "                'valid_price_ranges': (df['high'] >= df['low']).all(),\n",
    "                'close_in_range': ((df['close'] >= df['low']) & (df['close'] <= df['high'])).all(),\n",
    "                'positive_prices': (df[['open', 'high', 'low', 'close']] > 0).all().all(),\n",
    "                'sorted_chronologically': df.index.is_monotonic_increasing,\n",
    "                'has_basic_features': all(col in df.columns for col in ['returns_1d', 'volatility_7d', 'close_ma_7'])\n",
    "            }\n",
    "            \n",
    "            # Calculate overall validation score\n",
    "            validation_score = sum([v for k, v in checks.items() if isinstance(v, bool)]) / 6 * 100\n",
    "            checks['validation_score'] = validation_score\n",
    "            \n",
    "            validation_results.append(checks)\n",
    "            \n",
    "            # Print validation results\n",
    "            status = \"‚úÖ\" if validation_score == 100 else \"‚ö†Ô∏è\"\n",
    "            print(f\"{status} {symbol} {interval}: {validation_score:.0f}% validation score ({len(df):,} records)\")\n",
    "\n",
    "# Summary of validation\n",
    "validation_df = pd.DataFrame(validation_results)\n",
    "avg_validation_score = validation_df['validation_score'].mean()\n",
    "perfect_datasets = (validation_df['validation_score'] == 100).sum()\n",
    "\n",
    "print(f\"\\nüìä VALIDATION SUMMARY\")\n",
    "print(f\"Average Validation Score: {avg_validation_score:.1f}%\")\n",
    "print(f\"Perfect Datasets: {perfect_datasets}/{len(validation_df)}\")\n",
    "print(f\"Total Clean Records: {validation_df['total_records'].sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Save Cleaning Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save validation and quality results\n",
    "import os\n",
    "\n",
    "# Ensure directory exists\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Save quality assessment\n",
    "quality_df.to_csv('../data/processed/data_quality_assessment.csv', index=False)\n",
    "\n",
    "# Save validation results\n",
    "validation_df.to_csv('../data/processed/data_validation_results.csv', index=False)\n",
    "\n",
    "# Save processing summary\n",
    "summary_df.to_csv('../data/processed/processing_summary.csv', index=False)\n",
    "\n",
    "print(\"üíæ RESULTS SAVED\")\n",
    "print(\"=\" * 20)\n",
    "print(\"üìÅ data_quality_assessment.csv\")\n",
    "print(\"üìÅ data_validation_results.csv\")\n",
    "print(\"üìÅ processing_summary.csv\")\n",
    "print(\"üìÅ Individual processed datasets for each crypto/interval\")\n",
    "\n",
    "print(\"\\n‚úÖ Data cleaning and preprocessing completed successfully!\")\n",
    "print(\"‚û°Ô∏è Next step: Feature Engineering (03_feature_engineering.ipynb)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
